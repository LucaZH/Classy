{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,re,os\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file content function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_name):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean special characters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    content = re.sub(r'[^a-zA-Z\\s]|(\\d+\\w*)',' ', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter word by pos and append it in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_words(splited_text,pos,_list):\n",
    "    postag = nltk.pos_tag(splited_text)\n",
    "    \n",
    "    for pt in postag:\n",
    "        if pt[1] in pos:\n",
    "            _list.append(pt[0].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(file_path,_list):\n",
    "    pos = ['NN','NNP','NNS','VB','VBP','VBD','VBZ','VBN']\n",
    "    with open(file_path, 'r', encoding='utf-8') as text_file:\n",
    "            content = clean_text(text_file.read()).split()\n",
    "            filter_pos_words(content,pos,_list)\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(directory,vocabulaire):\n",
    "    for f in os.listdir(directory):\n",
    "        file_path = f'{directory}/{f}'\n",
    "        extract_words(file_path,vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all word in all files and add it into a list\n",
    "\n",
    "Retrieves the vocabulary from a given file and appends it to the provided list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = []\n",
    "get_vocabulary('./data/politics',vocabulaire)\n",
    "get_vocabulary('./data/economics',vocabulaire)\n",
    "get_vocabulary('./data/techno',vocabulaire)\n",
    "cleaned_vocabulaire = list(set(vocabulaire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21778\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_vocabulaire))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the presence of the words\n",
    "Check the presence of words in the header and return a list of binary values indicating the presence or absence of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_presence(header,word_list,_class):\n",
    "    res = []\n",
    "    for i in range(len(header)):\n",
    "        if header[i] in word_list:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    res.insert(0,_class)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_file(file_path):\n",
    "        words = []\n",
    "        pos = ['NN','NNP','NNS','VB','VBP','VBD','VBZ','VBN']\n",
    "        with open(file_path, 'r', encoding='utf-8') as text_file:\n",
    "                content = clean_text(text_file.read()).split()\n",
    "                filter_pos_words(content,pos,words)\n",
    "        return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_documents_in_folder(documents_words, directory, _class):\n",
    "        for f in os.listdir(directory):\n",
    "                file_path = f'{directory}/{f}'\n",
    "                doc_words = get_words_from_file(file_path)\n",
    "                documents_words.append(check_word_presence(cleaned_vocabulaire, doc_words, _class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_words = []\n",
    "check_all_documents_in_folder(documents_words, './data/economics', 'economics')\n",
    "check_all_documents_in_folder(documents_words, './data/politics', 'politics')\n",
    "check_all_documents_in_folder(documents_words, './data/techno', 'technologies')\n",
    "check_all_documents_in_folder(documents_words, './data/others', 'others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into a csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "\n",
    "cleaned_vocabulaire.insert(0, 'Classname')\n",
    "\n",
    "with open('../datasets/luca.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(cleaned_vocabulaire)\n",
    "    for docs in documents_words:\n",
    "        writer.writerow(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
